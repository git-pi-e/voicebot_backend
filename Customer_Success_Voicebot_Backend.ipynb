{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-pi-e/voicebot_backend/blob/main/Customer_Success_Voicebot_Backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backend for Voicebot\n",
        "\n",
        "## Instructions\n",
        "- Add your ngrok token as well as HuggingFace token to Google Colab secrets.\n",
        "- Run all the cells.\n",
        "- Pick up the ngrok Public URL and use it as the backend url for the deployed frontend (environment variable) or local server (.env.local -> NEXT_PUBLIC_BACKEND_URL)"
      ],
      "metadata": {
        "id": "2JDkVY2Fnsu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vX4zZfyEhmDi",
        "outputId": "53bb4649-b380-4b73-d104-3880793240bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.112.2)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.38.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.9)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn transformers\n",
        "!pip install nest-asyncio pyngrok\n",
        "!pip install python-multipart\n",
        "!pip install soundfile pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File, Body, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import io\n",
        "import requests\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import logging\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Enable CORS\n",
        "origins = [\"*\"]  # Adjust this as needed\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load models\n",
        "try:\n",
        "    stt_model = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny.en\")\n",
        "    logger.info(\"STT model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading STT model: {e}\")\n",
        "    stt_model = None\n",
        "\n",
        "try:\n",
        "    llm_model = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    logger.info(\"LLM model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading LLM model: {e}\")\n",
        "    llm_model = None\n",
        "\n",
        "# try:\n",
        "#     model = AutoModelForSeq2SeqLM.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
        "#     tts_model = ParlerTTSPipeline(model=model)\n",
        "#     logger.info(\"TTS model loaded successfully\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Error loading TTS model: {e}\")\n",
        "#     tts_model = None\n",
        "\n",
        "# Utility function to create a retryable session\n",
        "def create_retryable_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504)):\n",
        "    session = requests.Session()\n",
        "    retry = Retry(\n",
        "        total=retries,\n",
        "        read=retries,\n",
        "        connect=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "# Test route to ensure the API is running\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    logger.info(\"Root endpoint accessed\")\n",
        "    return {\"message\": \"Welcome to the Colab-hosted Voice Interaction API\"}\n",
        "\n",
        "# Transcription route with detailed error handling\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe_audio(file: UploadFile = File(...)):\n",
        "    if stt_model is None:\n",
        "        logger.error(\"STT model not available\")\n",
        "        raise HTTPException(status_code=500, detail=\"STT model not available\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting transcription process...\")\n",
        "        logger.info(f\"Received file: {file.filename}, Content Type: {file.content_type}\")\n",
        "\n",
        "        # Read the WAV file content\n",
        "        audio_content = await file.read()\n",
        "        logger.info(f\"Audio content size: {len(audio_content)} bytes\")\n",
        "\n",
        "        # Ensure the audio content is a valid WAV file\n",
        "        try:\n",
        "            # Use SoundFile to check the file and read data\n",
        "            wav_io = io.BytesIO(audio_content)\n",
        "            wav_io.seek(0)\n",
        "            data, samplerate = sf.read(wav_io)\n",
        "        except Exception as file_error:\n",
        "            logger.error(f\"Error reading WAV file: {file_error}\")\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid WAV file format. Please upload a valid WAV file.\")\n",
        "\n",
        "        # Transcribe audio using the loaded model\n",
        "        try:\n",
        "            transcript = stt_model(audio_content)[\"text\"]\n",
        "            logger.info(f\"Transcription result: {transcript}\")\n",
        "            return {\"transcript\": transcript}\n",
        "        except ValueError as model_error:\n",
        "            logger.error(f\"Model inference error: {model_error}\")\n",
        "            raise HTTPException(status_code=500, detail=\"Error during transcription. The audio may be malformed or corrupted.\")\n",
        "        except Exception as unknown_model_error:\n",
        "            logger.error(f\"Unexpected model error: {unknown_model_error}\", exc_info=True)\n",
        "            raise HTTPException(status_code=500, detail=\"An unexpected error occurred during transcription.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled exception: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"Error transcribing audio: {e}\")\n",
        "\n",
        "\n",
        "# Text generation route with detailed error handling\n",
        "@app.post(\"/generate\")\n",
        "async def generate_response(prompt: str = Body(...)):\n",
        "    if llm_model is None:\n",
        "        logger.error(\"LLM model not available\")\n",
        "        raise HTTPException(status_code=500, detail=\"LLM model not available\")\n",
        "    try:\n",
        "        logger.info(f\"Generating response for prompt: {prompt}\")\n",
        "\n",
        "        response = llm_model(prompt, max_length=100, num_return_sequences=1)\n",
        "        if not response or \"generated_text\" not in response[0]:\n",
        "            logger.error(\"Invalid response format from LLM model\")\n",
        "            raise HTTPException(status_code=502, detail=\"Failed to generate valid response\")\n",
        "\n",
        "        response_text = response[0]['generated_text']\n",
        "        logger.info(f\"Generated response: {response_text}\")\n",
        "        return {\"response\": response_text}\n",
        "    except requests.exceptions.RequestException as re:\n",
        "        logger.error(f\"RequestException: {re}\")\n",
        "        if isinstance(re, requests.exceptions.ConnectionError):\n",
        "            error_message = \"Connection to the LLM server failed. Please try again later.\"\n",
        "        elif isinstance(re, requests.exceptions.Timeout):\n",
        "            error_message = \"The request to the LLM server timed out. Please try again.\"\n",
        "        else:\n",
        "            error_message = \"An error occurred while communicating with the LLM server.\"\n",
        "        raise HTTPException(status_code=503, detail=error_message)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled exception during response generation: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"Error generating response: {e}\")\n",
        "\n",
        "# Speech synthesis route with detailed error handling\n",
        "# @app.post(\"/synthesize\")\n",
        "# async def synthesize_speech(text: str = Body(...)):\n",
        "#     if tts_model is None:\n",
        "#         logger.error(\"TTS model not available\")\n",
        "#         raise HTTPException(status_code=500, detail=\"TTS model not available\")\n",
        "#     try:\n",
        "#         logger.info(f\"Synthesizing speech for text: {text}\")\n",
        "#         audio_output = tts_model(text)\n",
        "#         if not audio_output:\n",
        "#             logger.error(\"Failed to generate audio output\")\n",
        "#             raise HTTPException(status_code=502, detail=\"Failed to synthesize audio\")\n",
        "\n",
        "#         logger.info(\"Speech synthesis successful\")\n",
        "#         return StreamingResponse(audio_output, media_type=\"audio/wav\")\n",
        "#     except requests.exceptions.RequestException as re:\n",
        "#         logger.error(f\"RequestException: {re}\")\n",
        "#         if isinstance(re, requests.exceptions.ConnectionError):\n",
        "#             error_message = \"Connection to the TTS server failed. Please try again later.\"\n",
        "#         elif isinstance(re, requests.exceptions.Timeout):\n",
        "#             error_message = \"The request to the TTS server timed out. Please try again.\"\n",
        "#         else:\n",
        "#             error_message = \"An error occurred while communicating with the TTS server.\"\n",
        "#         raise HTTPException(status_code=503, detail=error_message)\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Unhandled exception during speech synthesis: {e}\", exc_info=True)\n",
        "#         raise HTTPException(status_code=500, detail=f\"Error synthesizing speech: {e}\")\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "ngrok.set_auth_token(userdata.get('NGROK_AUTHTOKEN'))\n",
        "\n",
        "# This allows the FastAPI app to run within the notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Expose the app using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8000)\n",
        "    logger.info(f\"Public URL: {public_url}\")\n",
        "    logger.info(\"Colab-hosted Voice Interaction API is now running!\")\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error connecting to ngrok: {e}\")\n",
        "\n",
        "# Run the app\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TETGdQcIb9Vk",
        "outputId": "97ec4baa-056d-441d-de04-baee69d54f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-34' coro=<Server.serve() done, defined at /usr/local/lib/python3.10/dist-packages/uvicorn/server.py:67> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 577, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 68, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 328, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://069f-34-105-43-87.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [444]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     49.207.193.200:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     49.207.193.200:0 - \"OPTIONS /generate HTTP/1.1\" 200 OK\n",
            "INFO:     49.207.193.200:0 - \"POST /generate HTTP/1.1\" 422 Unprocessable Entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     49.207.193.200:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     49.207.193.200:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     49.207.193.200:0 - \"POST /generate HTTP/1.1\" 422 Unprocessable Entity\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxOwwV+VumPd53lk+4wATa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}