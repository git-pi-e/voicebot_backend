{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-pi-e/voicebot_backend/blob/main/Customer_Success_Voicebot_Backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JDkVY2Fnsu8"
      },
      "source": [
        "# Backend for Voicebot\n",
        "\n",
        "## Instructions\n",
        "- Add your ngrok token as well as HuggingFace token to Google Colab secrets.\n",
        "- Run all the cells.\n",
        "- Pick up the ngrok Public URL and use it as the backend url for the deployed frontend (environment variable) or local server (.env.local -> NEXT_PUBLIC_BACKEND_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vX4zZfyEhmDi",
        "outputId": "6e8943f7-1555-4e54-aa43-5b8e34d28a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.112.2 h11-0.14.0 starlette-0.38.2 uvicorn-0.30.6\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.9\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.7.4)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn transformers\n",
        "!pip install nest-asyncio pyngrok\n",
        "!pip install python-multipart\n",
        "!pip install soundfile pydub\n",
        "!pip install torch\n",
        "!pip install gTTS # Alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TETGdQcIb9Vk",
        "outputId": "f49c04d8-ecea-42b4-fab9-a5f87986b63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-28' coro=<Server.serve() done, defined at /usr/local/lib/python3.10/dist-packages/uvicorn/server.py:67> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 577, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 68, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 328, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://7fff-35-236-220-121.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [479]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription result:  Hello there.\n",
            "INFO:     49.207.235.88:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     49.207.235.88:0 - \"OPTIONS /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating response for prompt:  Hello there.\n",
            "Generated response: How are you?\n",
            "\n",
            "Jane:\n",
            "(smiling)\n",
            "I'm doing well, thank you. How about you?\n",
            "\n",
            "Tom:\n",
            "(smiling)\n",
            "I'm doing great. I'm just trying to get back into the swing of things after a long break.\n",
            "\n",
            "Jane:\n",
            "(nodding)\n",
            "I know how you feel. I've been off work for a while too.\n",
            "\n",
            "Tom:\n",
            "Text-to-speech conversion successful\n",
            "INFO:     49.207.235.88:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription result:  It will hurt.\n",
            "INFO:     49.207.235.88:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "Generating response for prompt:  It will hurt.\n",
            "Generated response: JASON: (sighs) I know. But I can't let it control me.\n",
            "\n",
            "SARAH: (smiling) You're not alone. We're here for you.\n",
            "\n",
            "JASON: (smiling back) Thanks, Sarah.\n",
            "\n",
            "SARAH: (nodding) We're here for you, Jason.\n",
            "\n",
            "JASON: (smiling) I\n",
            "Text-to-speech conversion successful\n",
            "INFO:     49.207.235.88:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription result:  Okay.\n",
            "INFO:     49.207.235.88:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "INFO:     49.207.235.88:0 - \"OPTIONS /generate HTTP/1.1\" 200 OK\n",
            "Generating response for prompt:  Okay.\n",
            "Generated response: So, let's say I have a list of strings, and I want to find the longest common substring between each pair of strings in the list.  How would I go about doing that?\n",
            "Text-to-speech conversion successful\n",
            "INFO:     49.207.235.88:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription result:  Are you sure about that?\n",
            "INFO:     49.207.235.88:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "Generating response for prompt:  Are you sure about that?\n",
            "Generated response: I mean, I've never been to a place like this before. It's so different from anything I've ever seen before.\n",
            "\n",
            "JASON: (smiling) Yeah, it's definitely different. But I'm excited to explore and learn about this place.\n",
            "\n",
            "SARAH: (nodding) Yeah, I'm excited too. I've always wanted to see something like this.\n",
            "Text-to-speech conversion successful\n",
            "INFO:     49.207.235.88:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, UploadFile, File, Body, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from transformers import pipeline\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from pyngrok import ngrok\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from gtts import gTTS\n",
        "from google.colab import userdata\n",
        "import soundfile as sf\n",
        "import io\n",
        "import requests\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import logging\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Enable CORS\n",
        "origins = [\"*\"]  # Adjust this as needed\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load models\n",
        "try:\n",
        "    stt_model = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny.en\")\n",
        "    logger.info(\"STT model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading STT model: {e}\")\n",
        "    stt_model = None\n",
        "\n",
        "try:\n",
        "    llm_model = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    logger.info(\"LLM model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading LLM model: {e}\")\n",
        "    llm_model = None\n",
        "\n",
        "# try:\n",
        "#     # Load the TTS model\n",
        "#     tts_tokenizer = T5Tokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
        "#     tts_model = T5ForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
        "#     logger.info(\"TTS model loaded successfully\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Error loading TTS model: {e}\")\n",
        "#     tts_model = None\n",
        "\n",
        "# Utility function to create a retryable session\n",
        "def create_retryable_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504)):\n",
        "    session = requests.Session()\n",
        "    retry = Retry(\n",
        "        total=retries,\n",
        "        read=retries,\n",
        "        connect=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "# Test route to ensure the API is running\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    logger.info(\"Root endpoint accessed\")\n",
        "    return {\"message\": \"Welcome to the Colab-hosted Voice Interaction API\"}\n",
        "\n",
        "# Transcription route with detailed error handling\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe_audio(file: UploadFile = File(...)):\n",
        "    if stt_model is None:\n",
        "        logger.error(\"STT model not available\")\n",
        "        raise HTTPException(status_code=500, detail=\"STT model not available\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting transcription process...\")\n",
        "        logger.info(f\"Received file: {file.filename}, Content Type: {file.content_type}\")\n",
        "\n",
        "        # Read the WAV file content\n",
        "        audio_content = await file.read()\n",
        "        logger.info(f\"Audio content size: {len(audio_content)} bytes\")\n",
        "\n",
        "        # Ensure the audio content is a valid WAV file\n",
        "        try:\n",
        "            # Use SoundFile to check the file and read data\n",
        "            wav_io = io.BytesIO(audio_content)\n",
        "            wav_io.seek(0)\n",
        "            data, samplerate = sf.read(wav_io)\n",
        "        except Exception as file_error:\n",
        "            logger.error(f\"Error reading WAV file: {file_error}\")\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid WAV file format. Please upload a valid WAV file.\")\n",
        "\n",
        "        # Transcribe audio using the loaded model\n",
        "        try:\n",
        "            transcript = stt_model(audio_content)[\"text\"]\n",
        "            logger.info(f\"Transcription result: {transcript}\")\n",
        "            print(f\"Transcription result: {transcript}\")\n",
        "            logger.info(\"Transcription successful\")\n",
        "\n",
        "\n",
        "            return {\"transcript\": transcript}\n",
        "        except ValueError as model_error:\n",
        "            logger.error(f\"Model inference error: {model_error}\")\n",
        "            raise HTTPException(status_code=500, detail=\"Error during transcription. The audio may be malformed or corrupted.\")\n",
        "        except Exception as unknown_model_error:\n",
        "            logger.error(f\"Unexpected model error: {unknown_model_error}\", exc_info=True)\n",
        "            raise HTTPException(status_code=500, detail=\"An unexpected error occurred during transcription.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled exception: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"Error transcribing audio: {e}\")\n",
        "\n",
        "# Text generation route with detailed error handling\n",
        "@app.post(\"/generate\")\n",
        "async def generate_response(data: dict = Body(...)):\n",
        "\n",
        "    prompt = data.get('prompt')\n",
        "    if not prompt:\n",
        "        raise HTTPException(status_code=400, detail=\"Prompt is required\")\n",
        "\n",
        "    if llm_model is None:\n",
        "        logger.error(\"LLM model not available\")\n",
        "        raise HTTPException(status_code=500, detail=\"LLM model not available\")\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Generating response for prompt: {prompt}\")\n",
        "        print(f\"Generating response for prompt: {prompt}\")\n",
        "\n",
        "        response = llm_model(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "        if not response or \"generated_text\" not in response[0]:\n",
        "            logger.error(\"Invalid response format from LLM model\")\n",
        "            raise HTTPException(status_code=502, detail=\"Failed to generate valid response\")\n",
        "\n",
        "        response_text = response[0]['generated_text']\n",
        "\n",
        "        # Remove the prompt from the generated response\n",
        "        response_text = response_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        logger.info(f\"Generated response: {response_text}\")\n",
        "        print(f\"Generated response: {response_text}\")\n",
        "        logger.info(\"Response generation successful\")\n",
        "\n",
        "        # Generate audio file using gTTS\n",
        "        tts = gTTS(text=response_text, lang='en')\n",
        "\n",
        "        # Use a unique identifier for the file name\n",
        "        filename = \"response_audio.mp3\"\n",
        "        tts.save(filename)\n",
        "        logger.info(\"Text-to-speech conversion successful\")\n",
        "        print(\"Text-to-speech conversion successful\")\n",
        "\n",
        "        # Option 1: Return base64 encoded audio in response\n",
        "        with open(filename, \"rb\") as audio_file:\n",
        "            audio_bytes = audio_file.read()\n",
        "            audio_base64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
        "\n",
        "        # Clean up the file after sending the response\n",
        "        os.remove(filename)\n",
        "\n",
        "        return {\n",
        "            \"response\": response_text,\n",
        "            \"audio_base64\": audio_base64\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as re:\n",
        "        logger.error(f\"RequestException: {re}\")\n",
        "        if isinstance(re, requests.exceptions.ConnectionError):\n",
        "            error_message = \"Connection to the LLM server failed. Please try again later.\"\n",
        "        elif isinstance(re, requests.exceptions.Timeout):\n",
        "            error_message = \"The request to the LLM server timed out. Please try again.\"\n",
        "        else:\n",
        "            error_message = \"An error occurred while communicating with the LLM server.\"\n",
        "        raise HTTPException(status_code=503, detail=error_message)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled exception during response generation: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"Error generating response: {e}\")\n",
        "\n",
        "# Speech synthesis route with detailed error handling\n",
        "# @app.post(\"/synthesize\")\n",
        "# async def synthesize_speech(text: str = Body(...)):\n",
        "#     if tts_model is None:\n",
        "#         logger.error(\"TTS model not available\")\n",
        "#         raise HTTPException(status_code=500, detail=\"TTS model not available\")\n",
        "#     try:\n",
        "#         logger.info(f\"Synthesizing speech for text: {text}\")\n",
        "\n",
        "#         # Tokenize and generate speech using the TTS model\n",
        "#         input_ids = tts_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "#         output = tts_model.generate(input_ids)\n",
        "#         audio_output = output[0].numpy()\n",
        "\n",
        "#         if audio_output is None:\n",
        "#             logger.error(\"Failed to generate audio output\")\n",
        "#             raise HTTPException(status_code=502, detail=\"Failed to synthesize audio\")\n",
        "\n",
        "#         logger.info(\"Speech synthesis successful\")\n",
        "\n",
        "#         # Stream the audio output\n",
        "#         return StreamingResponse(io.BytesIO(audio_output), media_type=\"audio/wav\")\n",
        "\n",
        "#     except requests.exceptions.RequestException as re:\n",
        "#         logger.error(f\"RequestException: {re}\")\n",
        "#         if isinstance(re, requests.exceptions.ConnectionError):\n",
        "#             error_message = \"Connection to the TTS server failed. Please try again later.\"\n",
        "#         elif isinstance(re, requests.exceptions.Timeout):\n",
        "#             error_message = \"The request to the TTS server timed out. Please try again.\"\n",
        "#         else:\n",
        "#             error_message = \"An error occurred while communicating with the TTS server.\"\n",
        "#         raise HTTPException(status_code=503, detail=error_message)\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Unhandled exception during speech synthesis: {e}\", exc_info=True)\n",
        "#         raise HTTPException(status_code=500, detail=f\"Error synthesizing speech: {e}\")\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "ngrok.set_auth_token(userdata.get('NGROK_AUTHTOKEN'))\n",
        "\n",
        "# This allows the FastAPI app to run within the notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Expose the app using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8000)\n",
        "    logger.info(f\"Public URL: {public_url}\")\n",
        "    logger.info(\"Colab-hosted Voice Interaction API is now running!\")\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error connecting to ngrok: {e}\")\n",
        "\n",
        "# Run the app\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTysawpMUkApbvb3kAonO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}